# -*- coding: utf-8 -*-
"""Analysing Nigerian Music Growth Globally.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qW14eCYpWvVLXyXYqCVcVNDCalQkHief

#**Installing The Neccesarry Packages**
"""

pip install spotipy

pip install matplotlib

pip install tabulate

"""#**Data Querying Using Spotify's API**"""

# Importing The Neccessary Libaries And Authroizing The Spotify Client

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
import pandas as pd
import matplotlib.pyplot as plt
import csv
import json
import datetime


client_id = '172421971086410d9e07c51d457e0751'
client_secret = '6f83b5d3a70d48cabccf86c76f4c2fe0'

# Authorize the client
client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)
sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)

from google.colab import drive
drive.mount('/content/drive')

# Creating a list of top 20 Nigerian artist to extract their streaming data

nigerian_artists = [
    'Wizkid',
    'Davido',
    'Burna Boy',
    'Tiwa Savage',
    'Olamide',
    'Yemi Alade',
    'Tekno',
    'Patoranking',
    'Flavour N abania',
    'Kizz Daniel',
    'Mr Eazi',
    'Simi',
    'Adekunle Gold',
    'Mayorkun',
    'Timaya',
    'Phyno',
    'Reekado Banks',
    'Niniola',
    'Tems',
    'Rema',
    'Asake',
    'Omah lay'
 ]

# Function to get each artist data such as id, name, top tracks, genres, and popularity

def get_artist_data(artist_name):
    results = sp.search(q=f'artist:{artist_name}', type='artist', limit=1)

    if results['artists']['items']:
        return results['artists']['items'][0]
    else:
        return None

def get_artist_top_tracks(artist_id):
    top_tracks = sp.artist_top_tracks(artist_id)
    return top_tracks['tracks']

artist_data = {}
for name in nigerian_artists:
    data = get_artist_data(name)

    if data:
        artist_data[data['id']] = {
            'name': data['name'],
            'id': data['id'],
            'genres': data['genres'],
            'popularity': data['popularity'],
            'top_tracks': get_artist_top_tracks(data['id'])
        }

# Function to get the each artist data from the year 2018 to 2022

from datetime import datetime as dt


def parse_date(date_str):
    for fmt in ('%Y-%m-%d', '%Y-%m', '%Y'):
        try:
            return dt.strptime(date_str, fmt)
        except ValueError:
            pass
    raise ValueError(f"Invalid date format: '{date_str}'")


def filter_tracks_by_release_date(tracks, start_date, end_date):
    filtered_tracks = []

    for track in tracks:
        release_date = parse_date(track['album']['release_date'])

        if release_date is not None and start_date <= release_date <= end_date:
            filtered_tracks.append(track)

    return filtered_tracks

start_date = dt.strptime('2018-01-01', '%Y-%m-%d')
end_date = dt.strptime('2022-12-31', '%Y-%m-%d')

for artist_id, artist_info in artist_data.items():
    artist_info['top_tracks_filtered'] = filter_tracks_by_release_date(artist_info['top_tracks'], start_date, end_date)

# Printing the results extracted

for artist_id, artist_info in artist_data.items():
    print(f"Artist: {artist_info['name']}\n")
    print("Top Tracks (2018-2022):")
    print("-" * 40)

    for track in artist_info['top_tracks_filtered']:
        print(f"Song Title: {track['name']}")
        print(f"Album: {track['album']['name']}")
        print(f"Release Date: {track['album']['release_date']}")
        print(f"Popularity: {track['popularity']}")
        print("-" * 40)

    print("\n")

# Converting the extracted datasets to tabular format for easier reading

from tabulate import tabulate

headers = ['Artist', 'Song Title', 'Album', 'Release Date', 'Popularity']
table_data = []

for artist_id, artist_info in artist_data.items():
    for track in artist_info['top_tracks_filtered']:
        row = [
            artist_info['name'],
            track['name'],
            track['album']['name'],
            track['album']['release_date'],
            track['popularity']
        ]
        table_data.append(row)

print(tabulate(table_data, headers=headers, tablefmt='grid'))

"""#**Visualizing and Analysing the Dataset**"""

# Importing the neccesary Data visualisation libaries

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

def plot_artist_popularity_grouped_bar_chart(artist_data, num_artists_to_display=10):
    # Limiting the number of artists to display
    limited_artist_data = {k: artist_data[k] for k in list(artist_data.keys())[:num_artists_to_display]}

    # Find the maximum number of top tracks
    max_num_top_tracks = max([len(artist_info['top_tracks_filtered']) for artist_info in limited_artist_data.values()])

    fig, ax = plt.subplots(figsize=(12, 6))

    # Setting the width of a bar
    bar_width = 0.1

    # Calculating the position of bars
    positions = list(range(max_num_top_tracks))

    for i, (artist_id, artist_info) in enumerate(limited_artist_data.items()):
        popularities = [track['popularity'] for track in artist_info['top_tracks_filtered']]

        # Padding the popularities list with zeros to make its length equal to max_num_top_tracks
        popularities.extend([0] * (max_num_top_tracks - len(popularities)))

        ax.bar([p + i * bar_width for p in positions], popularities, width=bar_width, label=artist_info['name'])

    ax.set_xlabel('Top Tracks')
    ax.set_ylabel('Popularity')
    ax.set_title('Artist Popularity Comparison (2018-2022)')
    ax.set_xticks([p + (num_artists_to_display / 2 - 0.5) * bar_width for p in positions])
    ax.set_xticklabels([f"Track {i + 1}" for i in range(max_num_top_tracks)])
    ax.legend()
    plt.xticks(rotation=45)
    plt.grid()
    plt.show()

plot_artist_popularity_grouped_bar_chart(artist_data)

#Function to calculate the artist popularity by year

def calculate_artist_popularity_by_year(artist_data, start_year, end_year):
    artist_popularity_by_year = {}
    years = range(start_year, end_year + 1)

    for artist_id, artist_info in artist_data.items():
        popularity_by_year = {year: [] for year in years}

        for track in artist_info['top_tracks_filtered']:
            release_date = parse_date(track['album']['release_date'])
            release_year = release_date.year
            if release_year in popularity_by_year:
                popularity_by_year[release_year].append(track['popularity'])

        avg_popularity_by_year = {year: np.mean(popularities) if popularities else 0 for year, popularities in popularity_by_year.items()}
        artist_popularity_by_year[artist_info['name']] = avg_popularity_by_year

    return artist_popularity_by_year

#Plotting the line chart

def plot_artist_popularity_trend(artist_data, num_artists_to_display=10):
    start_year = 2018
    end_year = 2022
    artist_popularity_by_year = calculate_artist_popularity_by_year(artist_data, start_year, end_year)

    fig, ax = plt.subplots(figsize=(12, 6))
    years = range(start_year, end_year + 1)

    for i, (artist_name, popularity_by_year) in enumerate(artist_popularity_by_year.items()):
        if i >= num_artists_to_display:
            break
        popularities = [popularity_by_year[year] for year in years]
        ax.plot(years, popularities, marker='o', label=artist_name)

    ax.set_xlabel('Years')
    ax.set_ylabel('Average Popularity')
    ax.set_title('Artist Popularity Trend (2018-2022)')
    ax.set_xticks(years)
    ax.legend()
    plt.grid()
    plt.show()

plot_artist_popularity_trend(artist_data)

"""#**Data Scraping from the official Uk Chart website to get chart data from the year 2018 to 2022**"""

# Importing the needed libraries

import requests, bs4
from datetime import datetime

# Function to get the chart links for the official uk singles chart from the year 2018 to 2022

def getalbums(url):
    allalbums = []
    print('Getting Page %s ' %url)
    req = requests.get(url)
    req.raise_for_status()

    # Exit loop if status code is not 200
    if req.status_code != 200:
        return None

    soup = bs4.BeautifulSoup(req.text,"lxml")

    # Retrieve chart dates and tidy the format
    sdate = soup.find_all("p", class_="article-date")
    date_str = sdate[0].text.strip().split(" - ")[0]
    date = datetime.strptime(date_str, "%d %B %Y")

    # Check if the year is 2023, then break the loop
    year = date.year
    if year >= 2023:
        return None

    # Retrieve album position, artist and album name
    positions = soup.find_all("span", class_="position")
    albums = soup.find_all("div", class_="title")
    artists = soup.find_all("div", class_="artist")

    # Create a list of each album, tidying the format
    for i in range(0,len(positions)):
        album = []
        album.append(date.strftime("%d/%m/%Y"))
        album.append(positions[i].text)
        album.append(artists[i].text.strip('\n').strip('\r'))
        album.append(albums[i].text.strip('\n').strip('\r'))

        # Append each album list to the weeks list
        allalbums.append(album)

    # Find next weeks information and create link, exit loop if link can't be found
    nextlink = soup.find("a",text="next")
    if nextlink == None:
        return None
    link = (nextlink['href'])
    link = 'http://www.officialcharts.com/' + link

    # Write weekly albums to CSV, appending to existing file
    with open("output.csv",'a',newline='') as resultFile:
        wr = csv.writer(resultFile)
        wr.writerows(allalbums)
        resultFile.close()

    # Clear out the weekly list and proceed to next weeks file
    allalbums = []
    getalbums(link)

# Enter start page to start the loop
getalbums('http://www.officialcharts.com/charts/singles-chart/20180105/7501/')

"""*This code retrieves single chart data from the first week of 2018 and stops at the last week of 2022. The output file will contain the single chart data for the specified period and saves the dataset into the **Output.csv** file.*"""

import time

def get_track_genre(artist, track, retries=3, base_delay=0.1):
    for attempt in range(retries):
        try:
            time.sleep(base_delay)
            results = sp.search(q=f'artist:{artist} track:{track}', type='track', limit=1)
            items = results['tracks']['items']

            if len(items) == 0:
                return "Not found"

            track_id = items[0]['id']
            artist_id = items[0]['artists'][0]['id']

            time.sleep(base_delay)
            artist = sp.artist(artist_id)
            genres = artist['genres']

            if len(genres) > 0:
                return ', '.join(genres)
            else:
                return "Unknown"

        except spotipy.SpotifyException as e:
            if e.http_status == 429:  # Rate limit error
                wait_time = int(e.headers.get('Retry-After', 1)) + 1
                print(f"Rate limit reached. Waiting for {wait_time} seconds before retrying.")
                time.sleep(wait_time)
            else:
                print(f"SpotifyException (HTTP status: {e.http_status}): {e.msg}")
        except Exception as e:
            print(f"Unexpected error occurred: {e}")

    return "Error"

with open('output.csv', 'r', newline='') as input_file:
    with open('output_with_genres.csv', 'w', newline='') as output_file:
        reader = csv.reader(input_file)
        writer = csv.writer(output_file)

        # Write header row
        writer.writerow(['Date', 'Position', 'Artist', 'Track', 'Genre'])

        for row in reader:
            date, position, artist, track = row
            genre = get_track_genre(artist, track)
            print(f"{track} by {artist} - Genre: {genre}")
            writer.writerow([date, position, artist, track, genre])

"""*This script reads the data from the **output.csv** file, fetches the genre information for each song using the Spotify API, and then saves the results in a new CSV file called **output_with_genres.csv**. The new file now has an additional column, 'Genre', containing the genre information for each song.*"""

output_df = pd.read_csv('output.csv')
output_with_genres_df = pd.read_csv('output_with_genres.csv')

"""*Due to Spotify API request cap, all genre information could not be extracted in one go.*"""

stopped_row = output_df[(output_df['Date'] == '01/01/2021') &
                        (output_df['Position'] == 99) &
                        (output_df['Artist'] == 'SHAWN MENDES') &
                        (output_df['Title'] == 'WONDER')].index[0]

remaining_data = output_df.iloc[stopped_row + 1:]

with open('output_with_genres.csv', 'a', newline='') as output_file:
    writer = csv.writer(output_file)

    for _, row in remaining_data.iterrows():
        date, position, artist, track = row['Date'], row['Position'], row['Artist'], row['Title']
        genre = get_track_genre(artist, track)
        print(f"{track} by {artist} - Genre: {genre}")
        writer.writerow([date, position, artist, track, genre])

"""*It was necessary to call in the first file, which contained the first batch of extracted dataset for the chart, and create a function to resume where it was interrupted.*

#**Visualising And Analysing The Dataset**
"""

# Filtered the dataset to only include the Nigerian artist present

artists_to_include = [
    'LOJAY & SARZ',
    'BURNA BOY',
    'WIZKID FT TEMS',
    'BURNA BOY FT ED SHEERAN',
    'BURNA BOY FT J HUS',
    'JAE5 FT DAVE & BNXN',
    'ASAKE',
    'WIZKID',
    'WIZKID FT AYRA STARR',
    'PHEELZ & BNXN',
    'OMAH LAY & JUSTIN BIEBER',
    'REMA',
    'CKAY'
]

filtered_df = output_with_genres_df[output_with_genres_df['Artist'].isin(artists_to_include)]
print(filtered_df)


filtered_df.to_csv('included_artists.csv', index=False)

artist_to_include = 'CKAY'

tem_artist_df = filtered_df[filtered_df['Artist'] == artist_to_include]

tem_artist_df = tem_artist_df.sort_values('Date') # Sorting the DataFrame by date

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'BURNA BOY'

tem_artist_df = filtered_df[filtered_df['Artist'] == artist_to_include]

tem_artist_df = tem_artist_df.sort_values('Date') # Sorting the DataFrame by date

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reversing the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotating the x-axis labels for better readability

plt.show()

artist_to_include = 'WIZKID FT TEMS'

tem_artist_df = filtered_df[filtered_df['Artist'] == artist_to_include]

tem_artist_df = tem_artist_df.sort_values('Date') # Sorting the DataFrame by date

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'BURNA BOY FT ED SHEERAN'

tem_artist_df = filtered_df[filtered_df['Artist'] == artist_to_include]

tem_artist_df = tem_artist_df.sort_values('Date') # Sorting the DataFrame by date

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'BURNA BOY FT J HUS'

tem_artist_df = filtered_df[filtered_df['Artist'] == artist_to_include]

tem_artist_df = tem_artist_df.sort_values('Date') # Sorting the DataFrame by date

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'JAE5 FT DAVE & BNXN'

tem_artist_df = filtered_df[filtered_df['Artist'] == artist_to_include]

tem_artist_df = tem_artist_df.sort_values('Date') # Sorting the DataFrame by date

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'ASAKE'

tem_artist_df = filtered_df[filtered_df['Artist'] == artist_to_include]

tem_artist_df = tem_artist_df.sort_values('Date') # Sorting the DataFrame by date

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'PHEELZ & BNXN'

tem_artist_df = filtered_df[filtered_df['Artist'] == artist_to_include]

tem_artist_df = tem_artist_df.sort_values('Date') # Sorting the DataFrame by date

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'OMAH LAY & JUSTIN BIEBER'

tem_artist_df = filtered_df[filtered_df['Artist'] == artist_to_include]

tem_artist_df = tem_artist_df.sort_values('Date') # Sorting the DataFrame by date

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'REMA'

tem_artist_df = filtered_df[filtered_df['Artist'] == artist_to_include]

tem_artist_df = tem_artist_df.sort_values('Date') # Sorting the DataFrame by date

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

"""#**Data Scraping from Billboard Hot 100 to get chart data from the year 2018 to 2022**"""

#installing the billboard library
!pip install billboard.py

#Importing the billboard package
import billboard
import datetime

start_date = datetime.date(2018, 1, 1)
end_date = datetime.date(2022, 12, 31)
current_date = start_date
chart_data = []

while current_date <= end_date:
    chart = billboard.ChartData('hot-100', date=current_date)

    for entry in chart:
        chart_data.append({
            'Date': current_date,
            'Position': entry.rank,
            'Artist': entry.artist,
            'Track': entry.title
        })

    current_date += datetime.timedelta(weeks=1)

billboard_df = pd.DataFrame(chart_data)
billboard_df.to_csv('billboard_hot_100.csv', index=False)

#Filtering out the nigerian artist present in the dataset

artists_to_include = [
    'Tems',
    'Wizkid Featuring Justin Bieber & Tems',
    'CKay',
    'Burna Boy',
    'Rema & Selena Gomez'
]


included_billboard_df = billboard_df[billboard_df['Artist'].isin(artists_to_include)]

# Save included data as CSV file
included_billboard_df.to_csv('included_billboard_hot_100.csv', index=False)

"""#**Visualizing and Analysing the Dataset**"""

artist_to_include = 'Tems'

tem_artist_df = billboard_df[billboard_df['Artist'] == artist_to_include]

tem_artist_df = tem_artist_df.sort_values('Date') # Sorting the DataFrame by date

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'Wizkid Featuring Justin Bieber & Tems'

tem_artist_df = billboard_df[billboard_df['Artist'] == artist_to_include]

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'CKay'

tem_artist_df = billboard_df[billboard_df['Artist'] == artist_to_include]

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'Burna Boy'

tem_artist_df = billboard_df[billboard_df['Artist'] == artist_to_include]

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability

plt.show()

artist_to_include = 'Rema & Selena Gomez'

tem_artist_df = billboard_df[billboard_df['Artist'] == artist_to_include]

plt.plot(tem_artist_df['Date'], tem_artist_df['Position'], label=artist_to_include)

plt.gca().invert_yaxis() # Reverse the order of the y-axis to show position increasing

plt.legend()
plt.xlabel('Date')
plt.ylabel('Position')
plt.title(f'Billboard Hot 100 Position by Artist and Track - {artist_to_include}')

plt.xticks(rotation=90) # Rotate the x-axis labels for better readability
plt.show()

"""#**Extracting audio features**"""

# Defining the tracks to search for
tracks = [
    "Burna Boy - Last Last",
    "Rema - Calm Down",
    "Wizkid - Essence (feat. Tems)",
    "Ckay - Love Wan Tin Tin (Ah Ah Ah)",
    "Tems - Free Mind",
    "Asake - Terminator",
    "Pheelz,BNXN fka Buju - Finesse",
    "Wizkid ft Ayra starr - 2 Sugar",
    "Omah Lay ft Justin Bieber - Attention",
    "Lojay,Sarz- Monalisa",
]

# Search for the tracks and obtain their IDs and other details
track_ids = []
track_details = []
for track in tracks:
    results = sp.search(q=track, limit=1, type='track')
    if results['tracks']['items']:
        track_item = results['tracks']['items'][0]
        track_id = track_item['id']
        track_ids.append(track_id)
        track_details.append({
            'artist': track_item['artists'][0]['name'],
            'track': track_item['name'],
        })
        print(f"Found track '{track}' with ID '{track_id}'")
    else:
        print(f"Track '{track}' not found")

# Get audio features for the obtained track IDs
audio_features = sp.audio_features(track_ids)

# Converting the audio features to a DataFrame
audio_features_df = pd.DataFrame(audio_features)

# Adding the artist and track columns to the DataFrame
audio_features_df['artist'] = [details['artist'] for details in track_details]
audio_features_df['track'] = [details['track'] for details in track_details]

# Rearranging the columns to show artist and track first
cols = audio_features_df.columns.tolist()
cols = cols[-2:] + cols[:-2]
audio_features_df = audio_features_df[cols]

# Save the DataFrame to a CSV file
audio_features_df.to_csv('audio_features_with_artist_track.csv', index=False)

# Display the DataFrame
print(tabulate(audio_features_df, headers='keys', tablefmt='pretty', showindex=False))

"""#**Analysising the audio features**"""

# Dropping specified columns with no numeric value from the DataFrame
audio_features_df = audio_features_df.drop(columns=['analysis_url', 'artist', 'id', 'track', 'track_href', 'type', 'uri'])

# Computing the descriptive statistics
mean = audio_features_df.mean()
median = audio_features_df.median()
mode = audio_features_df.mode().iloc[0]
std_dev = audio_features_df.std()

# Create a summary DataFrame
summary_stats = pd.DataFrame({'Mean': mean, 'Median': median, 'Mode': mode, 'Standard Deviation': std_dev})

# Display the summary statistics in a nicely formatted table
print(tabulate(summary_stats, headers='keys', tablefmt='pretty', showindex=True))

# Define the tracks to search for
tracks = [
    "Taylor Swift - Anti-Hero",
    "Drake - Toosie Slide",
    "Ariana Grande - pov",
    "Beyonce - Break My Soul",
    "Billie Eilish - 	Bad Guy",
    "Post Malone - Circles",
    "Ed Sheeran - Shape Of You",
    "Justin Bieber - Yummy",
    "Cardi B - Bodak Yellow (Money Moves)",
    "Chris Brown - Iffy",
]

# Search for the tracks and obtain their IDs and other details
track_ids = []
track_details = []
for track in tracks:
    results = sp.search(q=track, limit=1, type='track')
    if results['tracks']['items']:
        track_item = results['tracks']['items'][0]
        track_id = track_item['id']
        track_ids.append(track_id)
        track_details.append({
            'artist': track_item['artists'][0]['name'],
            'track': track_item['name'],
        })
        print(f"Found track '{track}' with ID '{track_id}'")
    else:
        print(f"Track '{track}' not found")

# Get audio features for the obtained track IDs
audio_features = sp.audio_features(track_ids)

# Converting the audio features to a DataFrame
audio_features_df2 = pd.DataFrame(audio_features)

# Add the artist and track columns to the DataFrame
audio_features_df2['artist'] = [details['artist'] for details in track_details]
audio_features_df2['track'] = [details['track'] for details in track_details]

# Rearrange the columns to show artist and track first
cols = audio_features_df2.columns.tolist()
cols = cols[-2:] + cols[:-2]
audio_features_df2 = audio_features_df2[cols]

# Save the DataFrame to a CSV file
audio_features_df2.to_csv('audio_features_with_artist_track2.csv', index=False)

# Display the DataFrame
print(tabulate(audio_features_df2, headers='keys', tablefmt='pretty', showindex=False))

# Dropping specified columns with no numeric value from the DataFrame
audio_features_df2 = audio_features_df2.drop(columns=['analysis_url', 'artist', 'id', 'track', 'track_href', 'type', 'uri'])

# Compute descriptive statistics
mean = audio_features_df2.mean()
median = audio_features_df2.median()
mode = audio_features_df2.mode().iloc[0]
std_dev = audio_features_df2.std()

# Create a summary DataFrame
summary_stats = pd.DataFrame({'Mean': mean, 'Median': median, 'Mode': mode, 'Standard Deviation': std_dev})

# Display the summary statistics in a nicely formatted table
print(tabulate(summary_stats, headers='keys', tablefmt='pretty', showindex=True))

# Add a country column to each dataset
audio_features_df['country'] = 'Nigeria'
audio_features_df2['country'] = 'US'

# Combining both datasets
combined_df = pd.concat([audio_features_df, audio_features_df2], ignore_index=True)

# Calculate the mean values of audio features for each country
mean_values = combined_df.groupby('country').mean()

#visualize

# List the features to compare
selected_features = ['danceability', 'energy', 'valence', 'acousticness', 'instrumentalness']  # Add more features if needed

fig, axes = plt.subplots(nrows=len(selected_features), ncols=1, figsize=(10, 6 * len(selected_features)))
for i, feature in enumerate(selected_features):
    sns.violinplot(x='country', y=feature, data=combined_df, ax=axes[i])
    axes[i].set_title(f'Comparison of {feature.capitalize()} Between Nigerian and US Artists')
plt.tight_layout()
plt.show()

"""#**Sentiment Analysis using twitter**"""

#installing the snsscrape library

!pip install snscrape nltk wordcloud

# Scraping the tweets using snscrape
import snscrape.modules.twitter as sntwitter

# Creating a list to append tweet data to
tweets = []

# Defining the search query and date range
query = "Afrobeats OR Nigerian music"
start_date = "2018-01-01"
end_date = "2022-12-31"

# Using TwitterSearchScraper to scrape data and append tweets to the list
for i, tweet in enumerate(sntwitter.TwitterSearchScraper(f'{query} since:{start_date} until:{end_date}').get_items()):
    if i > 50000:
        break
    tweets.append([tweet.user.username, tweet.date, tweet.likeCount, tweet.retweetCount, tweet.replyCount, tweet.lang,
                   tweet.sourceLabel, tweet.content])

# Creating a DataFrame to load the list
tweets_df = pd.DataFrame(tweets, columns=["user", "date_created", "number_of_likes", "number_of_retweets", "number_of_replies", "language",
                                          "source_of_tweet", "tweet"])

# Save the DataFrame as a CSV file
tweets_df.to_csv('afrobeats_nigerian_music_tweets.csv', index=False)

# Preprocessing the tweets
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

stop_words = set(stopwords.words('english'))

def preprocess_tweet(text):
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
    text = re.sub(r'\@\w+|\#', '', text)
    text = text.lower()
    text = re.sub(r'\s+[a-zA-Z]\s+', ' ', text)
    text = re.sub(r'\^[a-zA-Z]\s+', ' ', text)
    text = re.sub(r'\s+', ' ', text, flags=re.I)
    text = re.sub(r'^b\s+', '', text)
    text = re.sub(r'\W', ' ', text)

    words = nltk.word_tokenize(text)
    words = [word for word in words if word not in stop_words]
    return ' '.join(words)

tweets_df['Cleaned_Text'] = tweets_df['tweet'].apply(preprocess_tweet)

# Perform sentiment analysis using nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

sia = SentimentIntensityAnalyzer()

def get_sentiment(text):
    sentiment = sia.polarity_scores(text)
    return sentiment['compound']

tweets_df['Sentiment'] = tweets_df['Cleaned_Text'].apply(get_sentiment)

# Categorize sentiment scores
def categorize_sentiment(score):
    if score > 0:
        return 'positive'
    elif score < 0:
        return 'negative'
    else:
        return 'neutral'

tweets_df['Sentiment_Category'] = tweets_df['Sentiment'].apply(categorize_sentiment)

# Count the number of tweets in each sentiment category
sentiment_counts = tweets_df['Sentiment_Category'].value_counts()

# Plotting the pie chart
fig, ax = plt.subplots()
ax.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90)
ax.axis('equal')  # Equal aspect ratio ensures that the pie chart is circular.
plt.title('Sentiment Distribution')
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Combine all cleaned text into one string
all_words = ' '.join([text for text in tweets_df['Cleaned_Text']])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=800, background_color='white', stopwords=stop_words, min_font_size=10).generate(all_words)

# Plot the word cloud
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud)
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

"""#**Machine learning**

**Adding new columns for genre and audio features to the chart dataset. More Tracks were added to the dataset along the way**
"""

#defining the generes
afrobeat_genres = {'afropop', 'afro dancehall', 'nigerian pop'}

def is_afrobeat(artist_genres):
    return any(genre in afrobeat_genres for genre in artist_genres)

# Reading the CSV file
with open('output.csv', newline='') as csvfile:
    reader = csv.DictReader(csvfile)

    # Prepare to write the output to a new CSV file
    with open('output_with_features.csv', 'w', newline='') as outfile:
        fieldnames = reader.fieldnames + ['genre'] + list(sp.audio_features(['3tjFYV6RSFtuktYl3ZtYcq'])[0].keys())
        writer = csv.DictWriter(outfile, fieldnames=fieldnames)
        writer.writeheader()

        for row in reader:
            # Search for the track on Spotify
            track_info = sp.search(q=f'artist:{row["Artist"]} track:{row["Track"]}', type='track', limit=1)

            if track_info['tracks']['items']:
                track = track_info['tracks']['items'][0]
                track_id = track['id']
                artist_id = track['artists'][0]['id']

                # Get the artist's genres
                artist = sp.artist(artist_id)
                artist_genres = artist['genres']

                # Check if the track is afrobeat
                is_track_afrobeat = is_afrobeat(artist_genres)
                track_genre = 'Afrobeats' if is_track_afrobeat else 'Non-Afrobeats'

                # Get the audio features
                audio_features = sp.audio_features([track_id])[0]

                # Add the genre to the row
                row.update({'genre': track_genre})

                # Add the audio features to the row
                row.update(audio_features)
                writer.writerow(row)
                print(row)
            else:
                print(f'Track not found: {row["Artist"]} - {row["Track"]}')

afrobeat_genres = {'afropop', 'afro dancehall', 'nigerian pop'}

def is_afrobeat(genre_list):
    return any(genre in afrobeat_genres for genre in genre_list.split(', '))

# Read the input CSV file
with open('top_tracks_with_genres_audio_features.csv', newline='') as csvfile:
    reader = csv.DictReader(csvfile)

    # Prepare to write the output to a new CSV file
    with open('output_modified_genre.csv', 'w', newline='') as outfile:
        fieldnames = reader.fieldnames
        writer = csv.DictWriter(outfile, fieldnames=fieldnames)
        writer.writeheader()

        for row in reader:
            # Check if the track is afrobeat
            is_track_afrobeat = is_afrobeat(row['genres'])
            track_genre = 'Afrobeats' if is_track_afrobeat else 'Non-Afrobeats'

            # Replace the genre column value
            row['genres'] = track_genre

            # Write the modified row to the output CSV file
            writer.writerow(row)
            print(row)

"""**Merging the datasets**"""

# Read the first CSV file
with open('output_modified_genre.csv', newline='') as csvfile1:
    reader1 = csv.DictReader(csvfile1)
    fieldnames = reader1.fieldnames

    # Read the second CSV file
    with open('output_with_features.csv', newline='') as csvfile2:
        reader2 = csv.DictReader(csvfile2)

        # Check if both CSV files have the same columns
        if fieldnames != reader2.fieldnames:
            print("Error: The two CSV files have different columns")
        else:
            # Prepare to write the output to a new CSV file
            with open('merged_output.csv', 'w', newline='') as outfile:
                writer = csv.DictWriter(outfile, fieldnames=fieldnames)
                writer.writeheader()

                # Write rows from the first CSV file
                for row in reader1:
                    writer.writerow(row)

                # Write rows from the second CSV file
                for row in reader2:
                    writer.writerow(row)

            print("The two CSV files have been merged into 'merged_output.csv'")

# Get the playlist ID of "Top 100: Nigeria on Apple music"
playlist_name = 'Top 100: Nigeria on Apple music'
results = sp.search(q=playlist_name, type='playlist', limit=1)
playlist_id = results['playlists']['items'][0]['id']

# Get the tracks and audio features
offset = 0
tracks = []
while True:
    results = sp.playlist_tracks(playlist_id, offset=offset, fields='items.track.id, items.track.name, items.track.artists.name, total')
    if not results['items']:
        break
    tracks.extend(results['items'])
    offset += len(results['items'])

# Extract the relevant data and create the dataframe
data = []
for track in tracks:
    track_id = track['track']['id']
    track_name = track['track']['name']
    artist_name = ', '.join([artist['name'] for artist in track['track']['artists']])

    # Get audio features
    audio_features = sp.audio_features([track_id])[0]

    # Categorize genre
    genre = 'Afrobeats'

    data.append({
        'Artist': artist_name,
        'Track': track_name,
        'genre': genre,
        'danceability': audio_features['danceability'],
        'energy': audio_features['energy'],
        'key': audio_features['key'],
        'loudness': audio_features['loudness'],
        'mode': audio_features['mode'],
        'speechiness': audio_features['speechiness'],
        'acousticness': audio_features['acousticness'],
        'instrumentalness': audio_features['instrumentalness'],
        'liveness': audio_features['liveness'],
        'valence': audio_features['valence'],
        'tempo': audio_features['tempo'],
        'duration_ms': audio_features['duration_ms'],
        'time_signature': audio_features['time_signature'],
    })

df = pd.DataFrame(data)
df.to_csv('top_100_nigeria2.csv', index=False)

# Read the first CSV file
with open('merged_output.csv', newline='') as csvfile1:
    reader1 = csv.DictReader(csvfile1)
    fieldnames = reader1.fieldnames

    # Read the second CSV file
    with open('top_100_nigeria2.csv', newline='') as csvfile2:
        reader2 = csv.DictReader(csvfile2)

        # Check if both CSV files have the same columns
        if fieldnames != reader2.fieldnames:
            print("Error: The two CSV files have different columns")
        else:
            # Prepare to write the output to a new CSV file
            with open('merged_output1.csv', 'w', newline='') as outfile:
                writer = csv.DictWriter(outfile, fieldnames=fieldnames)
                writer.writeheader()

                # Write rows from the first CSV file
                for row in reader1:
                    writer.writerow(row)

                # Write rows from the second CSV file
                for row in reader2:
                    writer.writerow(row)

            print("The two CSV files have been merged into 'merged_output.csv'")

# Get the playlist ID of "Top 100: Nigeria on Apple music"
playlist_name = 'Chill RnB Playlist'
results = sp.search(q=playlist_name, type='playlist', limit=1)
playlist_id = results['playlists']['items'][0]['id']

# Get the tracks and audio features
offset = 0
tracks = []
while True:
    results = sp.playlist_tracks(playlist_id, offset=offset, fields='items.track.id, items.track.name, items.track.artists.name, total')
    if not results['items']:
        break
    tracks.extend(results['items'])
    offset += len(results['items'])

# Extract the relevant data and create the dataframe
data = []
for track in tracks:
    track_id = track['track']['id']
    track_name = track['track']['name']
    artist_name = ', '.join([artist['name'] for artist in track['track']['artists']])

    # Get audio features
    audio_features = sp.audio_features([track_id])[0]

    # Categorize genre
    genre = 'Non-Afrobeats'

    data.append({
        'Artist': artist_name,
        'Track': track_name,
        'genre': genre,
        'danceability': audio_features['danceability'],
        'energy': audio_features['energy'],
        'key': audio_features['key'],
        'loudness': audio_features['loudness'],
        'mode': audio_features['mode'],
        'speechiness': audio_features['speechiness'],
        'acousticness': audio_features['acousticness'],
        'instrumentalness': audio_features['instrumentalness'],
        'liveness': audio_features['liveness'],
        'valence': audio_features['valence'],
        'tempo': audio_features['tempo'],
        'duration_ms': audio_features['duration_ms'],
        'time_signature': audio_features['time_signature'],
    })

df = pd.DataFrame(data)
df.to_csv('Rnb1.csv', index=False)

"""**Merging the datasets**"""

# Read the first CSV file
with open('Rnb1.csv', newline='') as csvfile1:
    reader1 = csv.DictReader(csvfile1)
    fieldnames = reader1.fieldnames

    # Read the second CSV file
    with open('output_no_duplicates2.csv', newline='') as csvfile2:
        reader2 = csv.DictReader(csvfile2)

        # Check if both CSV files have the same columns
        if fieldnames != reader2.fieldnames:
            print("Error: The two CSV files have different columns")
        else:
            # Prepare to write the output to a new CSV file
            with open('New_merged_output3.csv', 'w', newline='') as outfile:
                writer = csv.DictWriter(outfile, fieldnames=fieldnames)
                writer.writeheader()

                # Write rows from the first CSV file
                for row in reader1:
                    writer.writerow(row)

                # Write rows from the second CSV file
                for row in reader2:
                    writer.writerow(row)

            print("The two CSV files have been merged into 'merged_output.csv'")

#Function to remove duplicate from the dataset

def remove_duplicates(input_file, output_file):
    with open(input_file, 'r', newline='', encoding='utf-8') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:
        reader = csv.reader(infile)
        writer = csv.writer(outfile)

        header = next(reader)  # Read the header row
        writer.writerow(header)  # Write the header row to the output file

        unique_rows = set()  # Initialize a set to store unique rows

        for row in reader:
            row_tuple = tuple(row)  # Convert the list to a tuple, as lists are unhashable
            if row_tuple not in unique_rows:
                unique_rows.add(row_tuple)
                writer.writerow(row)  # Write the unique row to the output file

if __name__ == '__main__':
    input_file = 'New_merged_output2.csv'
    output_file = 'output_no_duplicates2.csv'
    remove_duplicates(input_file, output_file)
    print('Duplicates removed successfully.')

#Function to count the Afrobeats and Non - Afrobeats songs present in the dataset

def count_genres(input_file):
    afrobeats_count = 0
    non_afrobeats_count = 0

    with open(input_file, 'r', newline='', encoding='utf-8') as infile:
        reader = csv.DictReader(infile)  # Use DictReader to access columns by name

        for row in reader:
            genre = row['genre']
            if genre == 'Afrobeats':
                afrobeats_count += 1
            elif genre == 'Non-Afrobeats':
                non_afrobeats_count += 1

    return afrobeats_count, non_afrobeats_count

if __name__ == '__main__':
    input_file = 'New_merged_output3.csv'
    afrobeats_count, non_afrobeats_count = count_genres(input_file)
    print(f"Afrobeats count: {afrobeats_count}")
    print(f"Non-Afrobeats count: {non_afrobeats_count}")

"""**Deploying a 1D CNN model to the dataset.**"""

pip install pandas numpy sklearn tensorflow

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

# Reading the dataset
data = pd.read_csv('New_merged_output3.csv')

print(tabulate(data.head(), headers='keys', tablefmt='pretty'))

# Count the total number of Afrobeats and Non-Afrobeats
genre_counts = data['genre'].value_counts()

# Create the bar chart
fig, ax = plt.subplots()
ax.barh(genre_counts.index, genre_counts.values, color=['blue', 'orange'])
ax.set_xlabel('Total Size')
ax.set_title('Total Size of Afrobeats and Non-Afrobeats')

# Display the bar chart
plt.show()

# Select the feature columns and target column
X = data.drop(['Artist', 'Track', 'genre'], axis=1)
y = data['genre']

# Standardize the feature data
scaler = StandardScaler()
X = scaler.fit_transform(X)

# One-hot encoding the target labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y)

# Spliting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Reshape the input data for 1D CNN
X_train = np.expand_dims(X_train, axis=2)
X_test = np.expand_dims(X_test, axis=2)

def create_model():
    model = Sequential()
    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
    model.add(Conv1D(128, kernel_size=3, activation='relu'))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(y_train.shape[1], activation='softmax'))

    model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

    return model

model = create_model()

history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)

test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test accuracy: {test_accuracy}")

"""**Predicting the genre on a fresh dataset of Nigerian music and Non Nigerian music using their audio features**"""

# Get the playlist ID of "Afrobeats Classics"
playlist_name = 'Afrobeats Classics'
results = sp.search(q=playlist_name, type='playlist', limit=1)
playlist_id = results['playlists']['items'][0]['id']

# Get the tracks and audio features
offset = 0
tracks = []
while True:
    results = sp.playlist_tracks(playlist_id, offset=offset, fields='items.track.id, items.track.name, items.track.artists.name, total')
    if not results['items']:
        break
    tracks.extend(results['items'])
    offset += len(results['items'])

# Extract the relevant data and create the dataframe
data = []
for track in tracks:
    track_id = track['track']['id']
    track_name = track['track']['name']
    artist_name = ', '.join([artist['name'] for artist in track['track']['artists']])

    # Get audio features
    audio_features = sp.audio_features([track_id])[0]

    # Categorize genre
    genre = 'Non-Afrobeats'

    data.append({
        'danceability': audio_features['danceability'],
        'energy': audio_features['energy'],
        'key': audio_features['key'],
        'loudness': audio_features['loudness'],
        'mode': audio_features['mode'],
        'speechiness': audio_features['speechiness'],
        'acousticness': audio_features['acousticness'],
        'instrumentalness': audio_features['instrumentalness'],
        'liveness': audio_features['liveness'],
        'valence': audio_features['valence'],
        'tempo': audio_features['tempo'],
        'duration_ms': audio_features['duration_ms'],
        'time_signature': audio_features['time_signature'],
    })

df = pd.DataFrame(data)
df.to_csv('Test1.csv', index=False)

# Get the playlist ID of "best rnb playlist ever fr"
playlist_name = 'best rnb playlist ever fr'
results = sp.search(q=playlist_name, type='playlist', limit=1)
playlist_id = results['playlists']['items'][0]['id']

# Get the tracks and audio features
offset = 0
tracks = []
while True:
    results = sp.playlist_tracks(playlist_id, offset=offset, fields='items.track.id, items.track.name, items.track.artists.name, total')
    if not results['items']:
        break
    tracks.extend(results['items'])
    offset += len(results['items'])

# Extract the relevant data and create the dataframe
data = []
for track in tracks:
    track_id = track['track']['id']
    track_name = track['track']['name']
    artist_name = ', '.join([artist['name'] for artist in track['track']['artists']])

    # Get audio features
    audio_features = sp.audio_features([track_id])[0]

    # Categorize genre
    genre = 'Non-Afrobeats'

    data.append({
        'danceability': audio_features['danceability'],
        'energy': audio_features['energy'],
        'key': audio_features['key'],
        'loudness': audio_features['loudness'],
        'mode': audio_features['mode'],
        'speechiness': audio_features['speechiness'],
        'acousticness': audio_features['acousticness'],
        'instrumentalness': audio_features['instrumentalness'],
        'liveness': audio_features['liveness'],
        'valence': audio_features['valence'],
        'tempo': audio_features['tempo'],
        'duration_ms': audio_features['duration_ms'],
        'time_signature': audio_features['time_signature'],
    })

df = pd.DataFrame(data)
df.to_csv('Test2.csv', index=False)

"""**Merging dataset**"""

# Read the first CSV file
with open('Test1.csv', newline='') as csvfile1:
    reader1 = csv.DictReader(csvfile1)
    fieldnames = reader1.fieldnames

    # Read the second CSV file
    with open('Test2.csv', newline='') as csvfile2:
        reader2 = csv.DictReader(csvfile2)

        # Check if both CSV files have the same columns
        if fieldnames != reader2.fieldnames:
            print("Error: The two CSV files have different columns")
        else:
            # Prepare to write the output to a new CSV file
            with open('Test_Data.csv', 'w', newline='') as outfile:
                writer = csv.DictWriter(outfile, fieldnames=fieldnames)
                writer.writeheader()

                # Write rows from the first CSV file
                for row in reader1:
                    writer.writerow(row)

                # Write rows from the second CSV file
                for row in reader2:
                    writer.writerow(row)

            print("The two CSV files have been merged into 'merged_output.csv'")

"""**Loading the dataset, depolying the trained model and predicting the genre**"""

# Load the new dataset
new_data = pd.read_csv('Test_Data.csv')

# Standardize the feature data using the same scaler used for the training data
X_new = scaler.transform(new_data)

# Reshape the input data for 1D CNN
X_new = np.expand_dims(X_new, axis=2)

# Predict the probabilities for each genre
predictions = model.predict(X_new)

# Get the genre index with the highest probability for each sample
predicted_indices = np.argmax(predictions, axis=1)

# Decode the genre indices to genre names
predicted_genres = label_encoder.inverse_transform(predicted_indices)

# Add the predicted genres to the new dataset
new_data['predicted_genre'] = predicted_genres

print(new_data)

print(tabulate(new_data.head(), headers='keys', tablefmt='pretty'))

new_data.to_csv('new_data_with_predicted_genres.csv', index=False)

"""**Deploying a Decision Tree Model**"""

from sklearn.tree import DecisionTreeClassifier, export_graphviz
import graphviz

data = pd.read_csv("New_merged_output3.csv")

# Extract the audio features (X) and the target labels (y)
X = data.drop(["Artist", "Track", "genre"], axis=1)
y = data["genre"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a Decision Tree classifier object
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training set
clf.fit(X_train, y_train)

# Calculate accuracy on the test set
accuracy = clf.score(X_test, y_test)
print(f"Test accuracy: {accuracy}")

# Export the Decision Tree as a Graphviz DOT format
dot_data = export_graphviz(clf, out_file=None, feature_names=X.columns, class_names=["Afrobeats", "Non-Afrobeats"], filled=True, rounded=True)

# Display the Decision Tree
graph = graphviz.Source(dot_data)
graph.view()

"""**Visualizing just the top of the decision tree by limiting the maximum depth for easy interpretation**"""

from sklearn.tree import DecisionTreeClassifier, plot_tree

# Assuming X and y are already defined
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the decision tree model
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)

# Plot the decision tree with a maximum depth of 3
plt.figure(figsize=(16, 10))
plot_tree(dt, feature_names=X.columns, class_names=["Afrobeats", "Non-Afrobeats"], max_depth=3, filled=True)
plt.show()

print(f"1D CNN Test accuracy: {test_accuracy}")
print(f"Decision Tree Test accuracy: {accuracy}")